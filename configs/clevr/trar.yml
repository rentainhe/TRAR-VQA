# Network
MODEL_USE: TRAR
LAYER: 6
HIDDEN_SIZE: 512
FF_SIZE: 2048
MULTI_HEAD: 8
DROPOUT_R: 0.1
FLAT_MLP_SIZE: 512
FLAT_GLIMPSES: 1
FLAT_OUT_SIZE: 1024

# TRAR Specific configs
ORDERS: [0, 1, 3]
IMG_SCALE: 14
ROUTING: 'attention' # 'soft' or 'hard'
POOLING: 'hard' # 'attention' or 'avg'
TAU_POLICY: 0 # SLOW 0, FAST 1, FINETUNE 2
TAU_MAX: 10
TAU_MIN: 0.1
BINARIZE: False

# Execution
BATCH_SIZE: 64
LR_BASE: 0.00004
LR_DECAY_R: 0.2
LR_DECAY_LIST: [13, 15]
WARMUP_EPOCH: 3
MAX_EPOCH: 16
GRAD_NORM_CLIP: -1
GRAD_ACCU_STEPS: 2
LOSS_FUNC: ce
LOSS_REDUCTION: sum
OPT: Adam
OPT_PARAMS: {betas: '(0.9, 0.98)', eps: '1e-9'}
